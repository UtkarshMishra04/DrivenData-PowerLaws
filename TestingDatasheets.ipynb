{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading complete\n",
      "                  timestamp  consumption  temperature\n",
      "0       2014-12-24 00:00:00    11.531180          NaN\n",
      "1       2014-12-24 01:00:00    11.570043          NaN\n",
      "2       2014-12-24 02:00:00    11.425395          NaN\n",
      "3       2014-12-24 03:00:00    11.456077          NaN\n",
      "4       2014-12-24 04:00:00    11.482227          NaN\n",
      "5       2014-12-24 05:00:00    11.600520          NaN\n",
      "6       2014-12-24 06:00:00    11.428562          NaN\n",
      "7       2014-12-24 07:00:00    11.521839          NaN\n",
      "8       2014-12-24 08:00:00    11.511144          NaN\n",
      "9       2014-12-24 09:00:00    11.420986          NaN\n",
      "10      2014-12-24 10:00:00    11.437447          NaN\n",
      "11      2014-12-24 11:00:00    11.360399          NaN\n",
      "12      2014-12-24 12:00:00    11.564806          NaN\n",
      "13      2014-12-24 13:00:00    11.347652          NaN\n",
      "14      2014-12-24 14:00:00    11.408107          NaN\n",
      "15      2014-12-24 15:00:00    11.476701          NaN\n",
      "16      2014-12-24 16:00:00    11.322000          NaN\n",
      "17      2014-12-24 17:00:00    11.472903          NaN\n",
      "18      2014-12-24 18:00:00    11.496051          NaN\n",
      "19      2014-12-24 19:00:00    11.473293          NaN\n",
      "20      2014-12-24 20:00:00    11.672180          NaN\n",
      "21      2014-12-24 21:00:00    11.466344          NaN\n",
      "22      2014-12-24 22:00:00    11.483000          NaN\n",
      "23      2014-12-24 23:00:00    11.478886          NaN\n",
      "24      2014-12-25 00:00:00    11.472952          NaN\n",
      "25      2014-12-25 01:00:00    11.473976          NaN\n",
      "26      2014-12-25 02:00:00    11.468894          NaN\n",
      "27      2014-12-25 03:00:00    11.678555          NaN\n",
      "28      2014-12-25 04:00:00    11.497433          NaN\n",
      "29      2014-12-25 05:00:00    11.599231          NaN\n",
      "...                     ...          ...          ...\n",
      "509346  2017-12-17 18:00:00    11.506609          NaN\n",
      "509347  2017-12-17 19:00:00    11.454109          NaN\n",
      "509348  2017-12-17 20:00:00     8.994240          NaN\n",
      "509349  2017-12-17 21:00:00     8.932812          NaN\n",
      "509350  2017-12-17 22:00:00     8.798748          NaN\n",
      "509351  2017-12-17 23:00:00     9.019047          NaN\n",
      "509352  2017-12-18 00:00:00     8.878572          NaN\n",
      "509353  2017-12-18 01:00:00     8.951884          NaN\n",
      "509354  2017-12-18 02:00:00     8.854683          NaN\n",
      "509355  2017-12-18 03:00:00     8.795903          NaN\n",
      "509356  2017-12-18 04:00:00     8.862710          NaN\n",
      "509357  2017-12-18 05:00:00     8.894188          NaN\n",
      "509358  2017-12-18 06:00:00     8.968203          NaN\n",
      "509359  2017-12-18 07:00:00     8.706558          NaN\n",
      "509360  2017-12-18 08:00:00    10.770894          NaN\n",
      "509361  2017-12-18 09:00:00    11.337012          NaN\n",
      "509362  2017-12-18 10:00:00    11.555814          NaN\n",
      "509363  2017-12-18 11:00:00    11.551069          NaN\n",
      "509364  2017-12-18 12:00:00    11.583514          NaN\n",
      "509365  2017-12-18 13:00:00    11.558695          NaN\n",
      "509366  2017-12-18 14:00:00    11.566799          NaN\n",
      "509367  2017-12-18 15:00:00    11.555814          NaN\n",
      "509368  2017-12-18 16:00:00    11.563181          NaN\n",
      "509369  2017-12-18 17:00:00    11.576959          NaN\n",
      "509370  2017-12-18 18:00:00    11.502861          NaN\n",
      "509371  2017-12-18 19:00:00    11.507366          NaN\n",
      "509372  2017-12-18 20:00:00     9.359175          NaN\n",
      "509373  2017-12-18 21:00:00     9.289487          NaN\n",
      "509374  2017-12-18 22:00:00     9.237278          NaN\n",
      "509375  2017-12-18 23:00:00     9.104205          NaN\n",
      "\n",
      "[509376 rows x 3 columns]\n",
      "                  timestamp  consumption  temperature\n",
      "0       2013-02-27 00:00:00     9.635330     2.833213\n",
      "1       2013-02-27 01:00:00     9.626627     2.904165\n",
      "2       2013-02-27 02:00:00     9.617289     2.890372\n",
      "3       2013-02-27 03:00:00     9.640200     2.833213\n",
      "4       2013-02-27 04:00:00     9.635811     2.827314\n",
      "5       2013-02-27 05:00:00     9.585563     2.833213\n",
      "6       2013-02-27 06:00:00    10.615161     2.833213\n",
      "7       2013-02-27 07:00:00    11.425449     2.827314\n",
      "8       2013-02-27 08:00:00    11.510712     2.833213\n",
      "9       2013-02-27 09:00:00    11.554983     2.944439\n",
      "10      2013-02-27 10:00:00    11.605943     3.046901\n",
      "11      2013-02-27 11:00:00    11.592678     3.044522\n",
      "12      2013-02-27 12:00:00    11.616535     3.135494\n",
      "13      2013-02-27 13:00:00    11.594706     3.182212\n",
      "14      2013-02-27 14:00:00    11.575773     3.218876\n",
      "15      2013-02-27 15:00:00    11.568028     3.218876\n",
      "16      2013-02-27 16:00:00    11.568755     3.186353\n",
      "17      2013-02-27 17:00:00    11.389236     3.135494\n",
      "18      2013-02-27 18:00:00    11.166036     3.178054\n",
      "19      2013-02-27 19:00:00    10.118730     3.077312\n",
      "20      2013-02-27 20:00:00     9.828288     3.044522\n",
      "21      2013-02-27 21:00:00     9.566297     2.944439\n",
      "22      2013-02-27 22:00:00     9.520619     2.901422\n",
      "23      2013-02-27 23:00:00     9.442862     2.944439\n",
      "24      2013-02-28 00:00:00     9.667139     2.944439\n",
      "25      2013-02-28 01:00:00     9.754533     2.887590\n",
      "26      2013-02-28 02:00:00     9.684441     2.833213\n",
      "27      2013-02-28 03:00:00     9.645660     2.833213\n",
      "28      2013-02-28 04:00:00     9.642456     2.781920\n",
      "29      2013-02-28 05:00:00     9.690446     2.833213\n",
      "...                     ...          ...          ...\n",
      "111954  2017-08-25 18:00:00    11.488025     3.455265\n",
      "111955  2017-08-25 19:00:00    11.377631     3.423176\n",
      "111956  2017-08-25 20:00:00    11.009580     3.356897\n",
      "111957  2017-08-25 21:00:00    10.741065     3.283414\n",
      "111958  2017-08-25 22:00:00    10.748094     3.218876\n",
      "111959  2017-08-25 23:00:00    10.730386     3.210844\n",
      "111960  2017-08-26 00:00:00    10.736696     3.178054\n",
      "111961  2017-08-26 01:00:00    10.739240     3.178054\n",
      "111962  2017-08-26 02:00:00    10.738525     3.157000\n",
      "111963  2017-08-26 03:00:00    10.741302     3.120895\n",
      "111964  2017-08-26 04:00:00    10.744230     3.120895\n",
      "111965  2017-08-26 05:00:00    10.741936     3.104587\n",
      "111966  2017-08-26 06:00:00    10.727819     3.120895\n",
      "111967  2017-08-26 07:00:00    11.827743     3.060271\n",
      "111968  2017-08-26 08:00:00    10.998368     3.190476\n",
      "111969  2017-08-26 09:00:00    11.199929     3.232121\n",
      "111970  2017-08-26 10:00:00    10.845870     3.258097\n",
      "111971  2017-08-26 11:00:00    11.167809     3.317816\n",
      "111972  2017-08-26 12:00:00    11.198526     3.378725\n",
      "111973  2017-08-26 13:00:00    11.225840     3.433987\n",
      "111974  2017-08-26 14:00:00    11.219131     3.465736\n",
      "111975  2017-08-26 15:00:00    11.293316     3.476099\n",
      "111976  2017-08-26 16:00:00    10.745730     3.476099\n",
      "111977  2017-08-26 17:00:00    10.743282     3.499533\n",
      "111978  2017-08-26 18:00:00    10.743202     3.496508\n",
      "111979  2017-08-26 19:00:00    10.732307     3.465736\n",
      "111980  2017-08-26 20:00:00    10.754997     3.414443\n",
      "111981  2017-08-26 21:00:00    10.766654     3.308107\n",
      "111982  2017-08-26 22:00:00    10.748173     3.295837\n",
      "111983  2017-08-26 23:00:00    10.743598     3.246491\n",
      "\n",
      "[111984 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "#Reading data\n",
    "\n",
    "df_training = pd.read_csv(\"Training_Data.csv\")\n",
    "df_leader = pd.read_csv(\"Test_Data.csv\")\n",
    "\n",
    "print(\"data reading complete\")\n",
    "\n",
    "#df_training = df_training.drop([' '],axis=1)\n",
    "df_training.drop(df_training.columns[[0,1]],axis=1,inplace=True)\n",
    "df_training_timestamp = df_training.loc[:, 'timestamp']\n",
    "df_leader_timestamp = df_leader.loc[:, 'timestamp']\n",
    "series_id = df_leader.loc[:, 'series_id']\n",
    "df_training.drop(df_training.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "df_leader.drop(df_leader.columns[[0,1,2]],axis=1,inplace=True)\n",
    "\n",
    "df_training = df_training.apply(np.log, axis=1)\n",
    "df_leader = df_leader.apply(np.log, axis=1)\n",
    "\n",
    "#df_training = np.log(df_training).dropna()\n",
    "\n",
    "df_training = pd.concat((df_training_timestamp,df_training),axis =1)\n",
    "df_leader = pd.concat((df_leader_timestamp,df_leader),axis =1)\n",
    "\n",
    "print(df_training)\n",
    "print(df_leader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all missing values with mean of existing values in training set\n",
    "#same can be done for testing set but data leakage can occur\n",
    "\n",
    "df_training = df_training.fillna(df_training.mean())\n",
    "\n",
    "df_leader = df_leader.fillna(df_leader.mean())\n",
    "\n",
    "print(\"data processed\")\n",
    "\n",
    "#df_training = (df_training - df_training.mean()) / (df_training.max() - df_training.min())\n",
    "#for head in df_training:\n",
    "    #df_training[head] = np.sqrt(df_training[head])\n",
    "\n",
    "#print(\"data normalised\")\n",
    "print(df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of people: 80000\n",
      "Number of features: 46\n"
     ]
    }
   ],
   "source": [
    "#general informations\n",
    "\n",
    "n_people = df_training.shape[0]\n",
    "n_features = df_training.shape[1]-1\n",
    "print(\"Total number of people: {}\".format(n_people))\n",
    "print(\"Number of features: {}\".format(n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['mvar1', 'mvar2', 'mvar3', 'mvar4', 'mvar5', 'mvar6', 'mvar7', 'mvar8', 'mvar9', 'mvar10', 'mvar11', 'mvar12', 'mvar13', 'mvar14', 'mvar15', 'mvar16', 'mvar17', 'mvar18', 'mvar19', 'mvar20', 'mvar21', 'mvar22', 'mvar23', 'mvar24', 'mvar25', 'mvar26', 'mvar27', 'mvar28', 'mvar29', 'mvar30', 'mvar31', 'mvar32', 'mvar33', 'mvar34', 'mvar35', 'mvar36', 'mvar37', 'mvar38', 'mvar39', 'mvar40', 'mvar41', 'mvar42', 'mvar43', 'mvar44', 'mvar45', 'mvar46']\n",
      "Target column: default_ind\n",
      "\n",
      "Feature values:-\n",
      "  mvar1   mvar2  mvar3  mvar4  mvar5 mvar6  mvar7 mvar8  mvar9 mvar10  ...    \\\n",
      "0  1696  1.6541  0.000    0.0    0.0     0   6015   322  40369  18414  ...     \n",
      "1  1846  0.8095  0.000    0.0    0.0   102   7532  3171  18234  13664  ...     \n",
      "2  1745  0.4001  0.000    0.0    0.0     0   2536     0      0   2536  ...     \n",
      "3  1739  0.2193  0.000    0.0    0.0  1982  26440  4955  20316  37013  ...     \n",
      "4  1787  0.0118  0.225    0.0    0.0  5451   5494  5494   7987   4696  ...     \n",
      "\n",
      "  mvar37 mvar38 mvar39   mvar40  mvar41   mvar42 mvar43   mvar44 mvar45 mvar46  \n",
      "0     10      4      1    73.78  82.547  0.08696     10  0.63899      0      0  \n",
      "1      0      2      0   99.129       0        0     13  0.63836      0      0  \n",
      "2      0      1      0        0   29.29        0      1  1.00000      0      0  \n",
      "3      3      2      0   96.272       0  0.15385      3  0.53241      0      0  \n",
      "4      3      2      0  115.019       0        0      1  0.92665      0      0  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "C:\\Users\\Utkarsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data normalised\n"
     ]
    }
   ],
   "source": [
    "#preparing data\n",
    "\n",
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(df_training.columns[:-1])  # all columns but last are features\n",
    "target_col = df_training.columns[-1] # last column is the target/label\n",
    "print(\"Feature column(s):-\\n{}\".format(feature_cols))\n",
    "print( \"Target column: {}\".format(target_col))\n",
    "\n",
    "feature_cols_leader = list(df_leader.columns[:])  # all columns but last are features\n",
    "\n",
    "X_all = df_training[feature_cols]  # feature values for all people\n",
    "y_all = df_training[target_col]  # corresponding targets/labels\n",
    "\n",
    "X_leader = df_leader[feature_cols_leader]\n",
    "\n",
    "print (\"\\nFeature values:-\")\n",
    "print(X_all.head())  # print the first 5 rows\n",
    "\n",
    "X_all = X_all.convert_objects(convert_numeric=True)\n",
    "X_leader = X_leader.convert_objects(convert_numeric=True)\n",
    "\n",
    "\n",
    "for head in X_all:\n",
    "    series = X_all[head]\n",
    "    mean = series.mean()\n",
    "    maximum = series.max()\n",
    "    minimum = series.min()\n",
    "    series = (series - minimum)/(maximum - minimum)\n",
    "    X_all[head] = series\n",
    "    \n",
    "for head in X_leader:\n",
    "    series = X_leader[head]\n",
    "    mean = series.mean()\n",
    "    maximum = series.max()\n",
    "    minimum = series.min()\n",
    "    series = (series - minimum)/(maximum - minimum)\n",
    "    X_leader[head] = series\n",
    "    \n",
    "\n",
    "print(\"data normalised\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (47):-\n",
      "['mvar1', 'mvar2', 'mvar3', 'mvar4', 'mvar5', 'mvar6', 'mvar7', 'mvar8', 'mvar9', 'mvar10', 'mvar11', 'mvar12', 'mvar13', 'mvar14', 'mvar15', 'mvar16', 'mvar17', 'mvar18', 'mvar19', 'mvar20', 'mvar21', 'mvar22', 'mvar23', 'mvar24', 'mvar25', 'mvar26', 'mvar27', 'mvar28', 'mvar29', 'mvar30', 'mvar31', 'mvar32', 'mvar33', 'mvar34', 'mvar35', 'mvar36', 'mvar37', 'mvar38', 'mvar39', 'mvar40', 'mvar41', 'mvar42', 'mvar43', 'mvar44', 'mvar45', 'mvar46', 'mvar47']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['C', 'L'], [1, 0])\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        #if col_data.dtype == object:\n",
    "            #col_data = pd.get_dummies(col_data, prefix=col)\n",
    "\n",
    "        outX = outX.join(col_data)  # collect columns in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "X_leader = preprocess_features(X_leader)\n",
    "mvar47 = mvar47.replace('C',0)\n",
    "mvar47 = mvar47.replace('L',1)\n",
    "mvar47leader = mvar47leader.replace('C',0)\n",
    "mvar47leader = mvar47leader.replace('L',1)\n",
    "\n",
    "X_all = pd.concat((X_all,mvar47),axis =1)\n",
    "X_leader = pd.concat((X_leader,mvar47leader),axis =1)\n",
    "\n",
    "print(\"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 72000 samples\n",
      "Test set: 8000 samples\n"
     ]
    }
   ],
   "source": [
    "# preparing number of training and test samples\n",
    "num_all = df_training.shape[0]  # same as len(df_training)\n",
    "num_train = 72000 # about 90% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "from sklearn import cross_validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, y_all, test_size=num_test)\n",
    "print(\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print (\"Test set: {} samples\".format(X_test.shape[0]))\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForestClassifier...\n",
      "Done!\n",
      "Training time (secs): 6.098\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print(\"Training {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nTraining time (secs): {:.3f}\".format(end - start))\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = ensemble.RandomForestClassifier(criterion='entropy',min_samples_split= 25)\n",
    "#clf = SVC(gamma='auto')\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using RandomForestClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.301\n",
      "F1 score for training set: 0.7765848013372766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print(\"Predicting labels using {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print( \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label=1)\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using RandomForestClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.062\n",
      "F1 score for test set: 0.5236818588025022\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print(\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "predict_leader = clf.predict(X_leader)\n",
    "\n",
    "data = {'default_ind':predict_leader}\n",
    "\n",
    "predict_leader = pd.DataFrame.from_dict(data)\n",
    "\n",
    "LeaderBoardFinal = pd.concat((application_key,predict_leader),axis =1)\n",
    "\n",
    "print(LeaderBoardFinal.shape)\n",
    "\n",
    "LeaderBoardFinal.to_csv('Flubbers_IITRookree_1.csv',index=False,header=False)\n",
    "\n",
    "\n",
    "#Now just by changing the classifier we can compare the f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 70000\n",
      "Training RandomForestClassifier...\n",
      "Done!\n",
      "Training time (secs): 5.942\n",
      "Predicting labels using RandomForestClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.300\n",
      "F1 score for training set: 0.7722247964519758\n",
      "Predicting labels using RandomForestClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.039\n",
      "F1 score for test set: 0.5013824884792627\n",
      "------------------------------------------\n",
      "Training set size: 60000\n",
      "Training RandomForestClassifier...\n",
      "Done!\n",
      "Training time (secs): 5.281\n",
      "Predicting labels using RandomForestClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.240\n",
      "F1 score for training set: 0.7703651957930687\n",
      "Predicting labels using RandomForestClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.031\n",
      "F1 score for test set: 0.5127105666156202\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"Training set size: {}\".format(len(X_train)))\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    print( \"F1 score for training set: {}\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "train_predict(clf, X_train.sample(n=70000, random_state=70000), y_train.sample(n=70000, random_state=70000), X_test, y_test)\n",
    "train_predict(clf, X_train.sample(n=60000, random_state=60000), y_train.sample(n=60000, random_state=60000), X_test, y_test)\n",
    "# Note: Keep the test set constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 72000\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 5.406\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.074\n",
      "F1 score for training set: 0.9038870069472354\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.008\n",
      "F1 score for test set: 0.45445292620865146\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.005\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for training set: 0.920353982300885\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.007\n",
      "F1 score for test set: 0.39015817223198596\n",
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.000\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 0.9545454545454545\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.45895806861499366\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier(min_samples_split=10)\n",
    "train_predict(dtc, X_train, y_train, X_test, y_test)\n",
    "train_predict(dtc, X_train.sample(n=200, random_state=202), y_train.sample(n=200, random_state=202), X_test, y_test)\n",
    "train_predict(dtc, X_train.sample(n=100, random_state=102), y_train.sample(n=100, random_state=102), X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#choosing the best model\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 'n_estimators': (100, 125, 150, 500)}\n",
    "sss = cross_validation.StratifiedShuffleSplit(y_train, test_size=num_test)\n",
    "gs = GridSearchCV(estimator=clf, n_jobs=-1, scoring=make_scorer(f1_score, pos_label=1), param_grid=parameters,\n",
    "                  cv=sss)\n",
    "gs.fit(X_train, y_train)\n",
    "best_estimator = gs.best_estimator_\n",
    "print(\"best estimator:\\n{}\".format(best_estimator))\n",
    "print ('')\n",
    "print(\"best parameter:\\n{}\".format(gs.best_params_))\n",
    "print('')\n",
    "print(\"F1 score:\\n{}\".format(f1_score(y_test, best_estimator.predict(X_test), pos_label=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
